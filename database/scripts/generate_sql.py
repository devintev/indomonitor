#!/usr/bin/env -S uv run --quiet --script
# /// script
# dependencies = [
#   "PyYAML>=6.0",
# ]
# ///
"""
Generate SQL schema from YAML schema definitions.

Reads field sets from database/schema/field_sets/*.yaml
and table definitions from database/schema/tables/news_monitoring_tables.yaml
to generate MySQL/MariaDB schema.

Output: database/generated/sql/schema.sql
"""

import yaml
from pathlib import Path
from typing import Any
from collections import defaultdict

# Template for generated file header
FILE_HEADER = '''-- =============================================================================
-- AUTO-GENERATED CODE - DO NOT EDIT
-- =============================================================================
-- ⚠️  WARNING: This file is automatically generated from YAML schema definitions
--
-- Source: ../../schema/tables/news_monitoring_tables.yaml
-- Field Sets: ../../schema/field_sets/*.yaml
-- Generator: ../../scripts/generate_sql.py
--
-- DO NOT EDIT THIS FILE MANUALLY
-- Any changes will be overwritten when code is regenerated.
--
-- To make changes:
-- 1. Edit YAML files in ../../schema/
-- 2. Run: ./database/scripts/generate_sql.py
-- 3. Generated code will update automatically
-- =============================================================================

-- =============================================================================
-- News Monitoring System Database Schema
-- =============================================================================
-- Database: indomonitor
-- Description: Complete schema for autonomous news monitoring with AI agents
-- =============================================================================

'''


def load_field_sets(field_sets_dir: Path) -> dict[str, dict]:
    """
    Load field sets from individual YAML files.

    Each file contains a single field set with structure:
    field_set:
      name: "set_name"
      description: "..."
      fields: [...]

    Returns:
        dict: {set_name: {description, fields}}
    """
    field_sets = {}

    if not field_sets_dir.exists():
        print(f"Warning: Field sets directory not found: {field_sets_dir}")
        return field_sets

    for yaml_file in field_sets_dir.glob("*.yaml"):
        try:
            with open(yaml_file, 'r') as f:
                data = yaml.safe_load(f)

            # Extract field_set (singular key)
            field_set = data.get("field_set")
            if field_set and "name" in field_set:
                name = field_set["name"]
                field_sets[name] = {
                    "description": field_set.get("description", ""),
                    "fields": field_set.get("fields", [])
                }
                print(f"  Loaded field set: {name} ({len(field_set.get('fields', []))} fields)")
            else:
                print(f"Warning: Invalid field set structure in {yaml_file}")

        except Exception as e:
            print(f"Error loading {yaml_file}: {e}")

    return field_sets


def load_table_schemas(tables_file: Path) -> dict[str, dict]:
    """
    Load table schemas from news_monitoring_tables.yaml.

    Returns:
        dict: {table_name: schema_definition}
    """
    if not tables_file.exists():
        raise FileNotFoundError(f"Table schemas file not found: {tables_file}")

    with open(tables_file, 'r') as f:
        data = yaml.safe_load(f)

    schemas = data.get("database_schemas", {})
    print(f"  Loaded {len(schemas)} table schemas")

    return schemas


def merge_field_sets_into_schema(schema: dict, field_sets: dict[str, dict]) -> dict:
    """
    Merge included field sets into a table schema.

    Field sets are added before table-specific fields, with deduplication.

    Args:
        schema: Table schema definition
        field_sets: Available field sets

    Returns:
        dict: Schema with field sets merged in
    """
    include_sets = schema.get("include_field_sets", [])

    if not include_sets:
        return schema

    # Collect all fields from included field sets
    merged_fields = []
    seen_field_names = set()

    for set_name in include_sets:
        if set_name not in field_sets:
            print(f"Warning: Field set '{set_name}' not found for table '{schema.get('db_table_name')}'")
            continue

        field_set = field_sets[set_name]
        for field in field_set.get("fields", []):
            field_name = field.get("name")
            if field_name and field_name not in seen_field_names:
                merged_fields.append(field)
                seen_field_names.add(field_name)

    # Add table-specific fields (skip duplicates)
    for field in schema.get("fields", []):
        field_name = field.get("name")
        if field_name and field_name not in seen_field_names:
            merged_fields.append(field)
            seen_field_names.add(field_name)

    # Update schema with merged fields
    schema_copy = schema.copy()
    schema_copy["fields"] = merged_fields

    return schema_copy


def get_sql_column_definition(field: dict) -> str:
    """
    Generate SQL column definition from YAML field.

    Args:
        field: Field definition from YAML

    Returns:
        str: SQL column definition (e.g., "name VARCHAR(255) NOT NULL")
    """
    field_name = field.get("name", "unknown")
    sql_type = field.get("sql_type", "")
    required = field.get("required", True)
    default_value = field.get("default")
    enum_values = field.get("enum")
    computed = field.get("computed", False)

    # Build column definition parts
    parts = [field_name]

    # Add SQL type (already fully specified in YAML)
    if sql_type:
        parts.append(sql_type)
    else:
        # Fallback if sql_type not specified
        parts.append("VARCHAR(255)")

    # Add NOT NULL constraint (skip for computed columns)
    if required and not computed and "AUTO_INCREMENT" not in sql_type:
        parts.append("NOT NULL")

    # Add DEFAULT value (skip for AUTO_INCREMENT and computed)
    if default_value and not computed and "AUTO_INCREMENT" not in sql_type:
        # Handle SQL expressions (don't quote)
        if isinstance(default_value, str) and ("CURRENT_TIMESTAMP" in default_value or "NOW()" in default_value):
            parts.append(f"DEFAULT {default_value}")
        # Handle numeric strings (don't quote)
        elif isinstance(default_value, str) and default_value.lstrip('-').replace('.', '', 1).isdigit():
            parts.append(f"DEFAULT {default_value}")
        # Handle string literals (quote)
        elif isinstance(default_value, str):
            parts.append(f"DEFAULT '{default_value}'")
        # Handle numeric values (don't quote)
        else:
            parts.append(f"DEFAULT {default_value}")

    # Add COMMENT for enum fields
    if enum_values:
        enum_str = ", ".join(enum_values)
        parts.append(f"COMMENT '{enum_str}'")

    return " ".join(parts)


def extract_foreign_keys(schema: dict) -> list[dict]:
    """
    Extract foreign key definitions from schema.

    Returns:
        list: [{'column': 'site_id', 'table': 'news_sites', 'ref_column': 'id', 'on_delete': 'CASCADE'}, ...]
    """
    foreign_keys = []

    for field in schema.get("fields", []):
        fk = field.get("foreign_key")
        if fk:
            foreign_keys.append({
                "column": field.get("name"),
                "table": fk.get("table"),
                "ref_column": fk.get("column", "id"),
                "on_delete": fk.get("on_delete", "RESTRICT")
            })

    return foreign_keys


def generate_indexes(schema: dict) -> list[str]:
    """
    Generate INDEX definitions from schema.

    Returns:
        list: ["INDEX idx_name (column)", "FULLTEXT INDEX ft_name (col1, col2)", ...]
    """
    indexes = []

    # Get index definitions from schema
    index_defs = schema.get("indexes", [])

    for idx in index_defs:
        idx_type = idx.get("type", "index").upper()  # Case-insensitive
        idx_name = idx.get("name")
        columns = idx.get("columns", [])

        if not columns:
            continue

        col_str = ", ".join(columns)

        if idx_type == "FULLTEXT":
            indexes.append(f"FULLTEXT INDEX {idx_name} ({col_str})")
        elif idx_type == "UNIQUE":
            indexes.append(f"UNIQUE KEY {idx_name} ({col_str})")
        else:
            indexes.append(f"INDEX {idx_name} ({col_str})")

    return indexes


def find_circular_foreign_keys(schemas: dict[str, dict]) -> dict[str, list]:
    """
    Identify circular foreign key dependencies that need ALTER TABLE.

    Returns:
        dict: {table_name: [list of FK defs to defer]}
    """
    # For simplicity, we know news_sites → scraper_scripts is circular
    # A more sophisticated approach would build a dependency graph

    circular_fks = {}

    # Check for news_sites → scraper_scripts circular dependency
    for table_name, schema in schemas.items():
        if table_name == "news_sites":
            fks = []
            for field in schema.get("fields", []):
                fk = field.get("foreign_key")
                if fk and fk.get("table") == "scraper_scripts":
                    fks.append({
                        "column": field.get("name"),
                        "table": fk.get("table"),
                        "ref_column": fk.get("column", "id"),
                        "on_delete": fk.get("on_delete", "SET NULL"),
                        "constraint_name": f"fk_{field.get('name')}"
                    })
            if fks:
                circular_fks["news_sites"] = fks

    return circular_fks


def generate_create_table(table_name: str, schema: dict, field_sets: dict, defer_fks: bool = False) -> str:
    """
    Generate CREATE TABLE statement.

    Args:
        table_name: Name of the table
        schema: Schema definition
        field_sets: Available field sets
        defer_fks: If True, skip foreign keys (for circular deps)

    Returns:
        str: Complete CREATE TABLE statement
    """
    # Merge field sets
    merged_schema = merge_field_sets_into_schema(schema, field_sets)

    # Extract metadata
    db_table_name = schema.get("db_table_name", table_name)
    title = schema.get("title", table_name)
    primary_key = schema.get("primary_key")

    # Start building SQL
    lines = []
    lines.append(f"-- {title}")
    lines.append(f"CREATE TABLE IF NOT EXISTS {db_table_name} (")

    # Generate column definitions
    column_defs = []
    for field in merged_schema.get("fields", []):
        col_def = get_sql_column_definition(field)

        # Add PRIMARY KEY designation
        if field.get("name") == primary_key and "AUTO_INCREMENT" in field.get("sql_type", ""):
            col_def += " PRIMARY KEY"
        elif field.get("name") == primary_key:
            col_def += " PRIMARY KEY"

        column_defs.append(f"    {col_def}")

    # Add foreign keys inline (unless deferred)
    if not defer_fks:
        for field in merged_schema.get("fields", []):
            fk = field.get("foreign_key")
            if fk:
                ref_table = fk.get("table")
                ref_column = fk.get("column", "id")
                on_delete = fk.get("on_delete", "RESTRICT")
                fk_line = f"    FOREIGN KEY ({field.get('name')}) REFERENCES {ref_table}({ref_column}) ON DELETE {on_delete}"
                column_defs.append(fk_line)

    # Generate indexes
    index_defs = generate_indexes(merged_schema)
    for idx in index_defs:
        column_defs.append(f"    {idx}")

    # Generate unique constraints
    unique_constraints = merged_schema.get("unique_constraints", [])
    for uc in unique_constraints:
        uc_name = uc.get("name")
        columns = uc.get("columns", [])
        if columns:
            col_str = ", ".join(columns)
            column_defs.append(f"    UNIQUE KEY {uc_name} ({col_str})")

    # Join all definitions
    lines.append(",\n".join(column_defs))

    # Close table definition
    lines.append(") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;")

    return "\n".join(lines)


def generate_alter_table_fks(table_name: str, fk_defs: list[dict]) -> str:
    """
    Generate ALTER TABLE statements for deferred foreign keys.

    Args:
        table_name: Table to alter
        fk_defs: List of FK definitions

    Returns:
        str: ALTER TABLE statements
    """
    lines = []
    lines.append(f"-- Add foreign key constraints to {table_name} after scraper_scripts exists")
    lines.append(f"ALTER TABLE {table_name}")

    alter_parts = []
    for fk in fk_defs:
        constraint_name = fk.get("constraint_name", f"fk_{fk['column']}")
        ref_table = fk["table"]
        ref_column = fk["ref_column"]
        alter_line = f"    ADD CONSTRAINT {constraint_name}\n    FOREIGN KEY ({fk['column']}) REFERENCES {ref_table}({ref_column}) ON DELETE {fk['on_delete']}"
        alter_parts.append(alter_line)

    lines.append(",\n".join(alter_parts) + ";")

    return "\n".join(lines)


def resolve_table_order(schemas: dict[str, dict]) -> list[str]:
    """
    Order tables by foreign key dependencies.

    Returns:
        list: Table names in dependency order
    """
    # Build dependency graph
    dependencies = defaultdict(set)

    for table_name, schema in schemas.items():
        for field in schema.get("fields", []):
            fk = field.get("foreign_key")
            if fk:
                ref_table = fk.get("table")
                if ref_table and ref_table != table_name:
                    dependencies[table_name].add(ref_table)

    # Topological sort (simple implementation)
    ordered = []
    remaining = set(schemas.keys())

    # Tables with no dependencies first
    while remaining:
        # Find tables with all dependencies satisfied
        ready = []
        for table in remaining:
            deps = dependencies.get(table, set())
            if all(dep in ordered or dep not in schemas for dep in deps):
                ready.append(table)

        if not ready:
            # Circular dependency - just add remaining in arbitrary order
            ready = list(remaining)

        for table in ready:
            ordered.append(table)
            remaining.remove(table)

    return ordered


def main():
    """Main generation logic."""
    print("\n=== SQL Schema Generator ===\n")

    # Setup paths
    script_dir = Path(__file__).parent
    database_dir = script_dir.parent
    schema_dir = database_dir / "schema"
    field_sets_dir = schema_dir / "field_sets"
    tables_file = schema_dir / "tables" / "news_monitoring_tables.yaml"
    output_file = database_dir / "generated" / "sql" / "schema.sql"

    print(f"Schema directory: {schema_dir}")
    print(f"Output file: {output_file}\n")

    # Load field sets
    print("Loading field sets...")
    field_sets = load_field_sets(field_sets_dir)
    print()

    # Load table schemas
    print("Loading table schemas...")
    schemas = load_table_schemas(tables_file)
    print()

    # Identify circular foreign keys
    print("Analyzing foreign key dependencies...")
    circular_fks = find_circular_foreign_keys(schemas)
    if circular_fks:
        print(f"  Found circular dependencies: {list(circular_fks.keys())}")
    print()

    # Resolve table order
    print("Resolving table creation order...")
    table_order = resolve_table_order(schemas)
    print(f"  Order: {' → '.join(table_order)}")
    print()

    # Generate CREATE TABLE statements
    print("Generating CREATE TABLE statements...")
    sql_statements = []

    for table_name in table_order:
        schema = schemas[table_name]
        defer_fks = table_name in circular_fks

        print(f"  Generating: {schema.get('db_table_name', table_name)}")

        create_stmt = generate_create_table(table_name, schema, field_sets, defer_fks)
        sql_statements.append(create_stmt)

    # Generate ALTER TABLE for circular FKs
    if circular_fks:
        print("\nGenerating ALTER TABLE for circular foreign keys...")
        for table_name, fk_defs in circular_fks.items():
            print(f"  Altering: {table_name}")
            alter_stmt = generate_alter_table_fks(table_name, fk_defs)
            sql_statements.append(alter_stmt)

    # Combine all SQL
    full_sql = FILE_HEADER + "\n\n".join(sql_statements) + "\n"

    # Ensure output directory exists
    output_file.parent.mkdir(parents=True, exist_ok=True)

    # Write output
    print(f"\nWriting SQL schema to: {output_file}")
    with open(output_file, 'w') as f:
        f.write(full_sql)

    print(f"\n✓ Successfully generated SQL schema")
    print(f"✓ Tables: {len(table_order)}")
    print(f"✓ Output: {output_file.relative_to(database_dir.parent)}")


if __name__ == "__main__":
    main()
